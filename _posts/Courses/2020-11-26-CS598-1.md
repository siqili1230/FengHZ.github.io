---
layout: post
title:  "Notes of CS598"
date:   2020-11-26 23:12:18 +0800
categories: Courses
tags: Notes, CS598
author: Zheng-Mao Zhu
mathjax: true
---

* content
{:toc}

# Notes of CS598





## Value Iteration
Note $\mathcal{T}$ as the bellman optimal operator that:

$$
(\mathcal{T} Q) (s,a) = R(s,a)+\gamma \mathbb{E}P(s'|s,a)[\arg\max_{a'}Q(s',a')]
$$

$$
Q^*=\mathcal{T} Q^*
$$

We have the algorithm that,

$$
Q^{*,0}=Q_0\\
Q^{*,h}=\mathcal{T} Q^{*,h-1}
$$

Q: If we denote $\pi_h:=\pi_{Q^{*,h}}$, and $Q^{\pi_h}$ be stationary point of operator $\mathcal{T}^{\pi_h}$ , then are $Q^{\pi_h}$ and $Q^{*,h}$ equivalent?

A: No. This problem equals that assume a policy $\pi$ is derived from $Q$, then whether $Q$ must be the converged Q function $Q^{\pi}$. It's obviously not.

